{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning in XGBoost using genetic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initilialize_poplulation(numberOfParents):\n",
    "    learningRate = np.empty([numberOfParents, 1])\n",
    "    nEstimators = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
    "    maxDepth = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
    "    minChildWeight = np.empty([numberOfParents, 1])\n",
    "    gammaValue = np.empty([numberOfParents, 1])\n",
    "    subSample = np.empty([numberOfParents, 1])\n",
    "    colSampleByTree =  np.empty([numberOfParents, 1])\n",
    "    for i in range(numberOfParents):\n",
    "        print(i)\n",
    "        learningRate[i] = round(random.uniform(0.01, 1), 2)\n",
    "        nEstimators[i] = random.randrange(10, 1500, step = 25)\n",
    "        maxDepth[i] = int(random.randrange(1, 10, step= 1))\n",
    "        minChildWeight[i] = round(random.uniform(0.01, 10.0), 2)\n",
    "        gammaValue[i] = round(random.uniform(0.01, 10.0), 2)\n",
    "        subSample[i] = round(random.uniform(0.01, 1.0), 2)\n",
    "        colSampleByTree[i] = round(random.uniform(0.01, 1.0), 2)\n",
    "    \n",
    "    population = np.concatenate((learningRate, nEstimators, maxDepth, minChildWeight, gammaValue, subSample, colSampleByTree), axis= 1)\n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parent Selection (survival of the fittest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_f1score(y_true, y_pred):\n",
    "    fitness = round((f1_score(y_true, y_pred, average='weighted')), 4)\n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the data annd find fitness score\n",
    "def train_population(population, dMatrixTrain, dMatrixtest, y_test):\n",
    "    fScore = []\n",
    "    for i in range(population.shape[0]):\n",
    "        param = { 'objective':'binary:logistic',\n",
    "              'learning_rate': population[i][0],\n",
    "              'n_estimators': population[i][1], \n",
    "              'max_depth': int(population[i][2]), \n",
    "              'min_child_weight': population[i][3],\n",
    "              'gamma': population[i][4], \n",
    "              'subsample': population[i][5],\n",
    "              'colsample_bytree': population[i][6],\n",
    "              'seed': 24}\n",
    "        num_round = 100\n",
    "        xgbT = xgb.train(param, dMatrixTrain, num_round)\n",
    "        preds = xgbT.predict(dMatrixtest)\n",
    "        preds = preds>0.5\n",
    "        fScore.append(fitness_f1score(y_test, preds))\n",
    "    return fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select parents for mating\n",
    "def new_parents_selection(population, fitness, numParents):\n",
    "    selectedParents = np.empty((numParents, population.shape[1])) #create an array to store fittest parents\n",
    "    \n",
    "    #find the top best performing parents\n",
    "    for parentId in range(numParents):\n",
    "        bestFitnessId = np.where(fitness == np.max(fitness))\n",
    "        bestFitnessId  = bestFitnessId[0][0]\n",
    "        selectedParents[parentId, :] = population[bestFitnessId, :]\n",
    "        fitness[bestFitnessId] = -1 #set this value to negative, in case of F1-score, so this parent is not selected again\n",
    "    return selectedParents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mate these parents to create children having parameters from these parents (we are using uniform crossover method)\n",
    "'''\n",
    "def crossover_uniform(parents, childrenSize):\n",
    "    \n",
    "    crossoverPointIndex = np.arange(0, np.uint8(childrenSize[1]), 1, dtype= np.uint8) #get all the index\n",
    "    crossoverPointIndex1 = np.random.randint(0, np.uint8(childrenSize[1]), np.uint8(childrenSize[1]/2)) # select half  of the indexes randomly\n",
    "    crossoverPointIndex2 = np.array(list(set(crossoverPointIndex) - set(crossoverPointIndex1))) #select leftover indexes\n",
    "    \n",
    "    children = np.empty(childrenSize)\n",
    "    \n",
    "    '''\n",
    "    Create child by choosing parameters from two parents selected using new_parent_selection function. The parameter values\n",
    "    will be picked from the indexes, which were randomly selected above. \n",
    "    '''\n",
    "    for i in range(childrenSize[0]):\n",
    "        \n",
    "        #find parent 1 index \n",
    "        parent1_index = i%parents.shape[0]\n",
    "        #find parent 2 index\n",
    "        parent2_index = (i+1)%parents.shape[0]\n",
    "        #insert parameters based on random selected indexes in parent 1\n",
    "        children[i, crossoverPointIndex1] = parents[parent1_index, crossoverPointIndex1]\n",
    "        #insert parameters based on random selected indexes in parent 1\n",
    "        children[i, crossoverPointIndex2] = parents[parent2_index, crossoverPointIndex2]\n",
    "    return children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutation(crossover, numberOfParameters):\n",
    "    #Define minimum and maximum values allowed for each parameter\n",
    "    minMaxValue = np.zeros((numberOfParameters, 2))\n",
    "    minMaxValue[0:] = [0.01, 1.0] #min/max learning rate\n",
    "    minMaxValue[1, :] = [10, 2000] #min/max n_estimator\n",
    "    minMaxValue[2, :] = [1, 15] #min/max depth\n",
    "    minMaxValue[3, :] = [0, 10.0] #min/max child_weight\n",
    "    minMaxValue[4, :] = [0.01, 10.0] #min/max gamma\n",
    "    minMaxValue[5, :] = [0.01, 1.0] #min/maxsubsample\n",
    "    minMaxValue[6, :] = [0.01, 1.0] #min/maxcolsample_bytree\n",
    " \n",
    "\n",
    "    # Mutation changes a single gene in each offspring randomly.\n",
    "    mutationValue = 0\n",
    "    parameterSelect = np.random.randint(0, 7, 1)\n",
    "    print(parameterSelect)\n",
    "    if parameterSelect == 0: #learning_rate\n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "    if parameterSelect == 1: #n_estimators\n",
    "        mutationValue = np.random.randint(-200, 200, 1)\n",
    "    if parameterSelect == 2: #max_depth\n",
    "        mutationValue = np.random.randint(-5, 5, 1)\n",
    "    if parameterSelect == 3: #min_child_weight\n",
    "        mutationValue = round(np.random.uniform(5, 5), 2)\n",
    "    if parameterSelect == 4: #gamma\n",
    "        mutationValue = round(np.random.uniform(-2, 2), 2)\n",
    "    if parameterSelect == 5: #subsample\n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "    if parameterSelect == 6: #colsample\n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "  \n",
    "#indtroduce mutation by changing one parameter, and set to max or min if it goes out of range\n",
    "    for idx in range(crossover.shape[0]):\n",
    "        crossover[idx, parameterSelect] = crossover[idx, parameterSelect] + mutationValue\n",
    "        if(crossover[idx, parameterSelect] > minMaxValue[parameterSelect, 1]):\n",
    "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 1]\n",
    "        if(crossover[idx, parameterSelect] < minMaxValue[parameterSelect, 0]):\n",
    "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 0]    \n",
    "        return crossover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12624\\2055721521.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(723)\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('clean2.data', header=None)\n",
    "X = dataset.iloc[:, 2:168].values #discard first two coloums as these are molecule's name and conformation's name\n",
    "y = dataset.iloc[:, 168].values #extrtact last coloum as class (1 => desired odor, 0 => undesired odor)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGboost Classifier\n",
    "\n",
    "#model xgboost\n",
    "#use xgboost API now\n",
    "xgDMatrix = xgb.DMatrix(X_train, y_train) #create Dmatrix\n",
    "xgbDMatrixTest = xgb.DMatrix(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'geneticXGboost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12624\\1276022744.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpopulationSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnumberOfParents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumberOfParameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#initialize the population with randomly generated parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mpopulation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgeneticXGboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitilialize_poplulation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumberOfParents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#define an array to store the fitness  hitory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mfitnessHistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumberOfGenerations\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumberOfParents\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'geneticXGboost' is not defined"
     ]
    }
   ],
   "source": [
    "numberOfParents = 8 #number of parents to start\n",
    "numberOfParentsMating = 4 #number of parents that will mate\n",
    "numberOfParameters = 7 #number of parameters that will be optimized\n",
    "numberOfGenerations = 4 #number of genration that will be created\n",
    "#define the population size\n",
    "populationSize = (numberOfParents, numberOfParameters)\n",
    "#initialize the population with randomly generated parameters\n",
    "population = geneticXGboost.initilialize_poplulation(numberOfParents)\n",
    "#define an array to store the fitness  hitory\n",
    "fitnessHistory = np.empty([numberOfGenerations+1, numberOfParents])\n",
    "#define an array to store the value of each parameter for each parent and generation\n",
    "populationHistory = np.empty([(numberOfGenerations+1)*numberOfParents, numberOfParameters])\n",
    "#insert the value of initial parameters in history\n",
    "populationHistory[0:numberOfParents, :] = population\n",
    "for generation in range(numberOfGenerations):\n",
    "    print(\"This is number %s generation\" % (generation))\n",
    "    \n",
    "    #train the dataset and obtain fitness\n",
    "    fitnessValue = geneticXGboost.train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)\n",
    "    fitnessHistory[generation, :] = fitnessValue\n",
    "    \n",
    "    #best score in the current iteration\n",
    "    print('Best F1 score in the this iteration = {}'.format(np.max(fitnessHistory[generation, :])))\n",
    "#survival of the fittest - take the top parents, based on the fitness value and number of parents needed to be selected\n",
    "    parents = geneticXGboost.new_parents_selection(population=population, fitness=fitnessValue, numParents=numberOfParentsMating)\n",
    "    \n",
    "    #mate these parents to create children having parameters from these parents (we are using uniform crossover)\n",
    "    children = geneticXGboost.crossover_uniform(parents=parents, childrenSize=(populationSize[0] - parents.shape[0], numberOfParameters))\n",
    "    \n",
    "    #add mutation to create genetic diversity\n",
    "    children_mutated = geneticXGboost.mutation(children, numberOfParameters)\n",
    "    \n",
    "    '''\n",
    "    We will create new population, which will contain parents that where selected previously based on the\n",
    "    fitness score and rest of them  will be children\n",
    "    '''\n",
    "    population[0:parents.shape[0], :] = parents #fittest parents\n",
    "    population[parents.shape[0]:, :] = children_mutated #children\n",
    "    \n",
    "    populationHistory[(generation+1)*numberOfParents : (generation+1)*numberOfParents+ numberOfParents , :] = population #srore parent information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best solution from the final iteration\n",
    "fitness = geneticXGboost.train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)\n",
    "fitnessHistory[generation+1, :] = fitness\n",
    "#index of the best solution\n",
    "bestFitnessIndex = np.where(fitness == np.max(fitness))[0][0]\n",
    "#Best fitness\n",
    "print(\"Best fitness is =\", fitness[bestFitnessIndex])\n",
    "#Best parameters\n",
    "print(\"Best parameters are:\")\n",
    "print('learning_rate', population[bestFitnessIndex][0])\n",
    "print('n_estimators', population[bestFitnessIndex][1])\n",
    "print('max_depth', int(population[bestFitnessIndex][2])) \n",
    "print('min_child_weight', population[bestFitnessIndex][3])\n",
    "print('gamma', population[bestFitnessIndex][4])\n",
    "print('subsample', population[bestFitnessIndex][5])\n",
    "print('colsample_bytree', population[bestFitnessIndex][6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
